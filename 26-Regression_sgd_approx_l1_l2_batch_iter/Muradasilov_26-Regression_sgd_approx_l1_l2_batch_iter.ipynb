{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tests import generate_regression_data, test_regression_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    '''Класс для предсказания действительно-значного выхода по входу - вектору из R^n.\n",
    "    Используется линейная регрессия, то есть если вход X \\in R^n, вектор весов W \\in R^{n+1},\n",
    "    то значение регрессии - это [X' 1] * W, то есть y = x1*w1 + x2*w2 + xn*wn + wn+1.\n",
    "    Обучение - подгонка весов W - будет вестись на парах (x, y).\n",
    "\n",
    "    Параметры\n",
    "    ----------\n",
    "    sgd : объект класса SGD\n",
    "    trainiterator: объект класса TrainIterator\n",
    "    n_epoch : количество эпох обучения (default = 1)\n",
    "    batch_size : размер пакета для шага SGD (default = 16)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, sgd, trainiterator, n_epoch=1, batch_size=16):\n",
    "        self.sgd = sgd\n",
    "        self.trainiterator = trainiterator\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Обучение модели.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "        y : массив/список правильных значений размера n_samples\n",
    "\n",
    "        Выход\n",
    "        -------\n",
    "        Метод обучает веса W\n",
    "        '''\n",
    "        random_index = random.randint(0, len(X) - 1)\n",
    "        self.W = X[random_index]\n",
    "\n",
    "        for subX, suby in self.trainiterator:\n",
    "            self.W = self.sgd.step(subX, suby, self.W)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Предсказание выходного значения для входных векторов\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "\n",
    "        Выход\n",
    "        -------\n",
    "        y : Массив размера n_samples\n",
    "        \"\"\"\n",
    "        y_pred = list()\n",
    "        y_pred = np.dot(X, self.W)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, y_gt, y_pred):\n",
    "        \"\"\"Возвращает точность регрессии в виде (1 - u/v),\n",
    "        где u - суммарный квадрат расхождения y_gt с y_pred,\n",
    "        v - суммарный квадрат расхождения y_gt с матожиданием y_gt\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        y_gt : массив/список правильных значений размера n_samples\n",
    "        y_pred : массив/список предсказанных значений размера n_samples\n",
    "\n",
    "        Выход\n",
    "        -------\n",
    "        accuracy - точность регрессии\n",
    "        \"\"\"\n",
    "        u, v, sum, N = 0, 0, 0, len(y_gt)\n",
    "        My_gt = y_gt.mean()\n",
    "\n",
    "        for i in range(N):\n",
    "            u += (y_gt[i] - y_pred[i]) ** 2\n",
    "            v += (y_gt[i] - My_gt) ** 2\n",
    "            \n",
    "        if N == 1:\n",
    "            v = 1\n",
    "\n",
    "        accuracy = 1 - u / v\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    '''Класс для реализации метода стохастического градиентного спуска.\n",
    "\n",
    "    Параметры\n",
    "    ----------\n",
    "    grad : функция вычисления градиента\n",
    "    alpha : градиентный шаг (default = 1.)\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, grad, alpha=1.):\n",
    "        self.grad = grad\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self, X, y, W):\n",
    "        '''Один шаг градиентного спуска.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "        y : массив/список правильных значений размера n_samples\n",
    "        W : массив весов размера n_weights\n",
    "\n",
    "        Выход\n",
    "        -------\n",
    "        Метод возвращает обновленные веса\n",
    "        '''\n",
    "        W_new = W - self.alpha * self.grad.grad(X, y, W) / len(X)\n",
    "\n",
    "        return W_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grad(object):\n",
    "    '''Класс для вычисления градиента по весам от функции потерь.\n",
    "\n",
    "    Параметры\n",
    "    ----------\n",
    "    loss : функция потерь\n",
    "    delta : параметр численного дифференцирования (default = 0.000001)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, loss, delta=0.000001):\n",
    "        self.loss = loss\n",
    "        self.delta = delta\n",
    "\n",
    "    def grad(self, X, y, W):\n",
    "        '''Вычисление градиента.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "        y : массив/список правильных значений размера n_samples\n",
    "        W : массив весов размера n_weights\n",
    "\n",
    "        Выход\n",
    "        -------\n",
    "        Метод возвращает градиент по весам W в точках X от функции потерь\n",
    "        '''\n",
    "        loss_gradient = np.zeros(len(W))\n",
    "        for i in range(len(loss_gradient)):\n",
    "            e = np.zeros(len(loss_gradient))\n",
    "            e[i] = 1\n",
    "            loss = self.loss.val(X, y, W)\n",
    "            loss_delta = self.loss.val(X, y, W + self.delta * e)\n",
    "            loss_gradient[i] = (loss_delta - loss) / self.delta\n",
    "\n",
    "        return loss_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''Класс для вычисления функции потерь.\n",
    "\n",
    "    Параметры\n",
    "    ----------\n",
    "    l1_coef : коэффициент l1 регуляризации (default = 0)\n",
    "    l2_coef : коэффициент l2 регуляризации (default = 0)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, l1_coef=0, l2_coef=0):\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "\n",
    "    def val(self, X, y, W):\n",
    "        '''Вычисление функции потерь.\n",
    "\n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "        y : массив/список правильных значений размера n_samples\n",
    "        W : массив весов размера n_weights\n",
    "\n",
    "        Выход\n",
    "        -------\n",
    "        Метод возвращает значение функции потерь в точках X\n",
    "        '''\n",
    "        loss, mse, cost = 0, 0, 0\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            mse += (np.dot(W, X[i]) - y[i]) ** 2\n",
    "\n",
    "        for i in range(len(W)):\n",
    "            cost += self.l1_coef * abs(W[i]) + self.l2_coef * (W[i]) ** 2\n",
    "\n",
    "        loss += mse + cost\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainIterator(object):\n",
    "    '''Класс итератора для работы с обучающими данными.\n",
    "\n",
    "    Параметры\n",
    "    ----------\n",
    "    X : двумерный массив признаков размера n_samples x n_features\n",
    "    y : массив/список правильных значений размера n_samples\n",
    "    n_epoch : количество эпох обучения (default = 1)\n",
    "    batch_size : размер пакета для шага SGD (default = 16)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, X, y, n_epoch=1, batch_size=16):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.counter = -1\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''Нужно для использования итератора в цикле for\n",
    "        Здесь ничего менять не надо\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        '''Выдача следующего батча.\n",
    "\n",
    "        Выход\n",
    "        -------\n",
    "        Метод возвращает очередной батч как из X, так и из y\n",
    "        '''\n",
    "        n_samples = len(self.X)\n",
    "        n_batches = (self.n_epoch * n_samples) // self.batch_size\n",
    "        self.counter += 1\n",
    "        if self.counter < n_batches:\n",
    "            i = self.counter * self.batch_size - (n_samples * ((self.counter * self.batch_size) // n_samples))\n",
    "            j = i + batch_size\n",
    "            k = j - n_samples if j > n_samples else -n_samples\n",
    "            return np.append(self.X[i:j], self.X[:k], axis=0), np.append(self.y[i:j], self.y[:k])\n",
    "        else:\n",
    "            raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В поисках лучшей точности для Nfeat=100, Mtrain=150, Mtest=150 основными параметрами были n_epoch и batch_size, так как остальные параметры довольны хрупкие и любое изменение в количестве нулей сильно влияет на ответ: например, при 10 эпохах и batch_size = 2 при alpha = 0.001, delta = 0.000001, l1_coef = 0.001, l2_coef = 0.001 точность была 0.58,\n",
    "             а при alpha = 0.01, delta = 0.0000001, l1_coef = 0.0001, l2_coef = 0.0001 была 0.87.\n",
    "Поэтому на остальных запусках использовал alpha = 0.01, delta = 0.0000001, l1_coef = 0.0001, l2_coef = 0.0001.\n",
    "С 50 < n_epoch < 100 при идеальном подборе остальных параметров (batch_size так, что бы было не менее 2 батчей за эпоху) точность менялась от 0.6 до 0.9. На batch_size = 2 эпохи показывали лучшую точность (0.91 при 15 эпохах, 0.97 при 30 эпохах, 0.98 при 60 эпохах) и это очевидно, так как итераций обновления весов происходит больше и веса обучаются лучше.\n",
    "На n_epoch = 100 с batch_size = 50 до batch_size = 5 (меньше не делал, так как работало долго) точность c 0.8 улучшалась до 0.98 соответсвенно.\n",
    "На n_epoch = 150 с batch_size = 5 точность улучшилась до 0.99.\n",
    "Увеличение эпох далее не помогало, так как на n_epoch = 200 с batch_size = 5 точность вышла 0.98.\n",
    "Таким образом по умолчанию я выбрал n_epoch = 60, batch_size = 2, так они показали наилучшую точность при наименьшем времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST REGRESSION MODEL: Your accuracy is 0.986970312514349\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 60\n",
    "batch_size = 2\n",
    "alpha = 0.01\n",
    "delta = 0.0000001\n",
    "l1_coef = 0.0001\n",
    "l2_coef = 0.0001\n",
    "\n",
    "trainX, trainY, testX, testY = generate_regression_data(Nfeat=100, Mtrain=150, Mtest=150)\n",
    "\n",
    "trainiterator = TrainIterator(trainX, trainY, n_epoch, batch_size)\n",
    "loss = Loss(l1_coef, l2_coef)\n",
    "grad = Grad(loss, delta)\n",
    "sgd = SGD(grad, alpha)\n",
    "\n",
    "reg = Regression(sgd, trainiterator, n_epoch, batch_size)\n",
    "\n",
    "test_regression_model(reg, trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
